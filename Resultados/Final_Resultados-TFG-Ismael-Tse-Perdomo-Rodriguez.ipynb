{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1i4fl7cWWFMZ1pQHmW3pCeAmu7hWDcIC3","timestamp":1720907468062}],"collapsed_sections":["L5lXfZbijpx_","vPoSTStAolSA","4HsflgU-2fOg","Thun3kXI2lTv","4D4hU4Qk8V7G","KFz4pwCq8fCv"],"mount_file_id":"1soIe9fsRaxL0w6mFvk30GFSc9V6lemLu","authorship_tag":"ABX9TyOqrpZeGxLnQgzhSYig/adO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install batchgenerators -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coq1MIOUmPV8","executionInfo":{"status":"ok","timestamp":1720928982685,"user_tz":-120,"elapsed":9887,"user":{"displayName":"Ismael","userId":"03967480790510634870"}},"outputId":"1c0af7d2-1b30-4019-e91c-549d3efd7752"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import os\n","import matplotlib\n","#from batchgenerators.utilities.file_and_folder_operations import join\n","matplotlib.use('agg')\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from collections import OrderedDict\n","import numpy as np\n","import cv2"],"metadata":{"id":"4XomZZj0jj_u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Curvas de aprendizaje"],"metadata":{"id":"L5lXfZbijpx_"}},{"cell_type":"code","source":["def plot_progress_png(output_path, epoca, train_losses, val_losses, mean_dice, ema_dice):\n","        # we infer the epoch form our internal logging\n","        epoch = epoca\n","        sns.set(font_scale=2.5)\n","        fig, ax = plt.subplots(figsize=(30, 18))\n","        # regular progress.png as we are used to from previous nnU-Net versions\n","        ax2 = ax.twinx()\n","        x_values = list(range(epoch + 1))\n","        ax.plot(x_values, train_losses[:epoch + 1], color='b', ls='-', label=\"loss_tr\", linewidth=4)\n","        ax.plot(x_values, val_losses[:epoch + 1], color='r', ls='-', label=\"loss_val\", linewidth=4)\n","        ax2.plot(x_values, mean_dice[:epoch + 1], color='g', ls='dotted', label=\"pseudo dice\",\n","                 linewidth=3)\n","        ax2.plot(x_values, ema_dice[:epoch + 1], color='g', ls='-', label=\"pseudo dice (mov. avg.)\",\n","                 linewidth=4)\n","        ax.set_xlabel(\"epoch\")\n","        ax.set_ylabel(\"loss\")\n","        ax2.set_ylabel(\"pseudo dice\")\n","        ax.legend(loc=(0, 1))\n","        ax2.legend(loc=(0.2, 1))\n","\n","        plt.tight_layout()\n","\n","        fig.savefig(join(output_path))\n","        plt.close()"],"metadata":{"id":"5WPuSwxWmVnE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_uformer = torch.load('/content/drive/MyDrive/TFG/Uformer/V2/logs/CholecSeg8k/Uformer_T_/models/model_final.pth', map_location=torch.device('cpu'))\n","checkpoint_reprec = torch.load('/content/drive/MyDrive/TFG/Parte2/FineTune/logs/CholecSeg8k/models/model_latest.pth', map_location=torch.device('cpu'))"],"metadata":{"id":"RDdehTRzmiyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_progress_png('/content/drive/MyDrive/TFG/Resultados/learning_curves/uformer_learning-curve.png', checkpoint_uformer['epoch'], checkpoint_uformer['train_losses'], checkpoint_uformer['val_losses'], checkpoint_uformer['mean_dice'], checkpoint_uformer['ema_dice'])\n","plot_progress_png('/content/drive/MyDrive/TFG/Resultados/learning_curves/reprec_learning-curve.png', checkpoint_reprec['epoch'], checkpoint_reprec['train_losses'], checkpoint_reprec['val_losses'], checkpoint_reprec['mean_dice'], checkpoint_reprec['ema_dice'])"],"metadata":{"id":"jq7AiXH0mz2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Métricas"],"metadata":{"id":"vPoSTStAolSA"}},{"cell_type":"markdown","source":["## Funciones Previas"],"metadata":{"id":"4HsflgU-2fOg"}},{"cell_type":"code","source":["from abc import ABC, abstractmethod\n","from typing import Tuple, Union, List\n","import numpy as np\n","\n","\n","class BaseReaderWriter(ABC):\n","    @staticmethod\n","    def _check_all_same(input_list):\n","        if len(input_list) == 1:\n","            return True\n","        else:\n","            # compare all entries to the first\n","            return np.allclose(input_list[0], input_list[1:])\n","\n","    @staticmethod\n","    def _check_all_same_array(input_list):\n","        # compare all entries to the first\n","        for i in input_list[1:]:\n","            if i.shape != input_list[0].shape or not np.allclose(i, input_list[0]):\n","                return False\n","        return True\n","\n","    @abstractmethod\n","    def read_images(self, image_fnames: Union[List[str], Tuple[str, ...]]) -> Tuple[np.ndarray, dict]:\n","        \"\"\"\n","        Reads a sequence of images and returns a 4d (!) np.ndarray along with a dictionary. The 4d array must have the\n","        modalities (or color channels, or however you would like to call them) in its first axis, followed by the\n","        spatial dimensions (so shape must be c,x,y,z where c is the number of modalities (can be 1)).\n","        Use the dictionary to store necessary meta information that is lost when converting to numpy arrays, for\n","        example the Spacing, Orientation and Direction of the image. This dictionary will be handed over to write_seg\n","        for exporting the predicted segmentations, so make sure you have everything you need in there!\n","\n","        IMPORTANT: dict MUST have a 'spacing' key with a tuple/list of length 3 with the voxel spacing of the np.ndarray.\n","        Example: my_dict = {'spacing': (3, 0.5, 0.5), ...}. This is needed for planning and\n","        preprocessing. The ordering of the numbers must correspond to the axis ordering in the returned numpy array. So\n","        if the array has shape c,x,y,z and the spacing is (a,b,c) then a must be the spacing of x, b the spacing of y\n","        and c the spacing of z.\n","\n","        In the case of 2D images, the returned array should have shape (c, 1, x, y) and the spacing should be\n","        (999, sp_x, sp_y). Make sure 999 is larger than sp_x and sp_y! Example: shape=(3, 1, 224, 224),\n","        spacing=(999, 1, 1)\n","\n","        For images that don't have a spacing, set the spacing to 1 (2d exception with 999 for the first axis still applies!)\n","\n","        :param image_fnames:\n","        :return:\n","            1) a np.ndarray of shape (c, x, y, z) where c is the number of image channels (can be 1) and x, y, z are\n","            the spatial dimensions (set x=1 for 2D! Example: (3, 1, 224, 224) for RGB image).\n","            2) a dictionary with metadata. This can be anything. BUT it HAS to include a {'spacing': (a, b, c)} where a\n","            is the spacing of x, b of y and c of z! If an image doesn't have spacing, just set this to 1. For 2D, set\n","            a=999 (largest spacing value! Make it larger than b and c)\n","\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def read_seg(self, seg_fname: str) -> Tuple[np.ndarray, dict]:\n","        \"\"\"\n","        Same requirements as BaseReaderWriter.read_image. Returned segmentations must have shape 1,x,y,z. Multiple\n","        segmentations are not (yet?) allowed\n","\n","        If images and segmentations can be read the same way you can just `return self.read_image((image_fname,))`\n","        :param seg_fname:\n","        :return:\n","            1) a np.ndarray of shape (1, x, y, z) where x, y, z are\n","            the spatial dimensions (set x=1 for 2D! Example: (1, 1, 224, 224) for 2D segmentation).\n","            2) a dictionary with metadata. This can be anything. BUT it HAS to include a {'spacing': (a, b, c)} where a\n","            is the spacing of x, b of y and c of z! If an image doesn't have spacing, just set this to 1. For 2D, set\n","            a=999 (largest spacing value! Make it larger than b and c)\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def write_seg(self, seg: np.ndarray, output_fname: str, properties: dict) -> None:\n","        \"\"\"\n","        Export the predicted segmentation to the desired file format. The given seg array will have the same shape and\n","        orientation as the corresponding image data, so you don't need to do any resampling or whatever. Just save :-)\n","\n","        properties is the same dictionary you created during read_images/read_seg so you can use the information here\n","        to restore metadata\n","\n","        IMPORTANT: Segmentations are always 3D! If your input images were 2d then the segmentation will have shape\n","        1,x,y. You need to catch that and export accordingly (for 2d images you need to convert the 3d segmentation\n","        to 2d via seg = seg[0])!\n","\n","        :param seg: A segmentation (np.ndarray, integer) of shape (x, y, z). For 2D segmentations this will be (1, y, z)!\n","        :param output_fname:\n","        :param properties: the dictionary that you created in read_images (the ones this segmentation is based on).\n","        Use this to restore metadata\n","        :return:\n","        \"\"\"\n","        pass"],"metadata":{"id":"yEoJ3DpFokt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Tuple, Union, List\n","import numpy as np\n","from skimage import io\n","\n","\n","class NaturalImage2DIO(BaseReaderWriter):\n","    \"\"\"\n","    ONLY SUPPORTS 2D IMAGES!!!\n","    \"\"\"\n","\n","    # there are surely more we could add here. Everything that can be read by skimage.io should be supported\n","    supported_file_endings = [\n","        '.png',\n","        # '.jpg',\n","        # '.jpeg', # jpg not supported because we cannot allow lossy compression! segmentation maps!\n","        '.bmp',\n","        '.tif'\n","    ]\n","\n","    def read_images(self, image_fnames: Union[List[str], Tuple[str, ...]]) -> Tuple[np.ndarray, dict]:\n","        images = []\n","        for f in image_fnames:\n","            npy_img = io.imread(f)\n","            if npy_img.ndim == 3:\n","                # rgb image, last dimension should be the color channel and the size of that channel should be 3\n","                # (or 4 if we have alpha)\n","                assert npy_img.shape[-1] == 3 or npy_img.shape[-1] == 4, \"If image has three dimensions then the last \" \\\n","                                                                         \"dimension must have shape 3 or 4 \" \\\n","                                                                         f\"(RGB or RGBA). Image shape here is {npy_img.shape}\"\n","                # move RGB(A) to front, add additional dim so that we have shape (c, 1, X, Y), where c is either 3 or 4\n","                images.append(npy_img.transpose((2, 0, 1))[:, None])\n","            elif npy_img.ndim == 2:\n","                # grayscale image\n","                images.append(npy_img[None, None])\n","\n","        if not self._check_all_same([i.shape for i in images]):\n","            print('ERROR! Not all input images have the same shape!')\n","            print('Shapes:')\n","            print([i.shape for i in images])\n","            print('Image files:')\n","            print(image_fnames)\n","            raise RuntimeError()\n","        return np.vstack(images, dtype=np.float32, casting='unsafe'), {'spacing': (999, 1, 1)}\n","\n","    def read_seg(self, seg_fname: str) -> Tuple[np.ndarray, dict]:\n","        return self.read_images((seg_fname, ))\n","\n","    def write_seg(self, seg: np.ndarray, output_fname: str, properties: dict) -> None:\n","        io.imsave(output_fname, seg[0].astype(np.uint8, copy=False), check_contrast=False)"],"metadata":{"id":"9TdKM-dSv-Rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections.abc import Iterable\n","\n","import numpy as np\n","import torch\n","\n","\n","def recursive_fix_for_json_export(my_dict: dict):\n","    # json is ... a very nice thing to have\n","    # 'cannot serialize object of type bool_/int64/float64'. Apart from that of course...\n","    keys = list(my_dict.keys())  # cannot iterate over keys() if we change keys....\n","    for k in keys:\n","        if isinstance(k, (np.int64, np.int32, np.int8, np.uint8)):\n","            tmp = my_dict[k]\n","            del my_dict[k]\n","            my_dict[int(k)] = tmp\n","            del tmp\n","            k = int(k)\n","\n","        if isinstance(my_dict[k], dict):\n","            recursive_fix_for_json_export(my_dict[k])\n","        elif isinstance(my_dict[k], np.ndarray):\n","            assert my_dict[k].ndim == 1, 'only 1d arrays are supported'\n","            my_dict[k] = fix_types_iterable(my_dict[k], output_type=list)\n","        elif isinstance(my_dict[k], (np.bool_,)):\n","            my_dict[k] = bool(my_dict[k])\n","        elif isinstance(my_dict[k], (np.int64, np.int32, np.int8, np.uint8)):\n","            my_dict[k] = int(my_dict[k])\n","        elif isinstance(my_dict[k], (np.float32, np.float64, np.float16)):\n","            my_dict[k] = float(my_dict[k])\n","        elif isinstance(my_dict[k], list):\n","            my_dict[k] = fix_types_iterable(my_dict[k], output_type=type(my_dict[k]))\n","        elif isinstance(my_dict[k], tuple):\n","            my_dict[k] = fix_types_iterable(my_dict[k], output_type=tuple)\n","        elif isinstance(my_dict[k], torch.device):\n","            my_dict[k] = str(my_dict[k])\n","        else:\n","            pass  # pray it can be serialized\n"],"metadata":{"id":"TidslZFJyTIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import multiprocessing\n","import os\n","from copy import deepcopy\n","from multiprocessing import Pool\n","from typing import Tuple, List, Union, Optional\n","\n","import numpy as np\n","from batchgenerators.utilities.file_and_folder_operations import subfiles, join, save_json, load_json, \\\n","    isfile\n","\n","def label_or_region_to_key(label_or_region: Union[int, Tuple[int]]):\n","    return str(label_or_region)\n","\n","\n","def key_to_label_or_region(key: str):\n","    try:\n","        return int(key)\n","    except ValueError:\n","        key = key.replace('(', '')\n","        key = key.replace(')', '')\n","        split = key.split(',')\n","        return tuple([int(i) for i in split if len(i) > 0])\n","\n","\n","def save_summary_json(results: dict, output_file: str):\n","    \"\"\"\n","    json does not support tuples as keys (why does it have to be so shitty) so we need to convert that shit\n","    ourselves\n","    \"\"\"\n","    results_converted = deepcopy(results)\n","    # convert keys in mean metrics\n","    results_converted['mean'] = {label_or_region_to_key(k): results['mean'][k] for k in results['mean'].keys()}\n","    # convert metric_per_case\n","    for i in range(len(results_converted[\"metric_per_case\"])):\n","        results_converted[\"metric_per_case\"][i]['metrics'] = \\\n","            {label_or_region_to_key(k): results[\"metric_per_case\"][i]['metrics'][k]\n","             for k in results[\"metric_per_case\"][i]['metrics'].keys()}\n","    # sort_keys=True will make foreground_mean the first entry and thus easy to spot\n","    save_json(results_converted, output_file, sort_keys=True)\n","\n","\n","def load_summary_json(filename: str):\n","    results = load_json(filename)\n","    # convert keys in mean metrics\n","    results['mean'] = {key_to_label_or_region(k): results['mean'][k] for k in results['mean'].keys()}\n","    # convert metric_per_case\n","    for i in range(len(results[\"metric_per_case\"])):\n","        results[\"metric_per_case\"][i]['metrics'] = \\\n","            {key_to_label_or_region(k): results[\"metric_per_case\"][i]['metrics'][k]\n","             for k in results[\"metric_per_case\"][i]['metrics'].keys()}\n","    return results\n","\n","\n","def labels_to_list_of_regions(labels: List[int]):\n","    return [(i,) for i in labels]\n","\n","\n","def region_or_label_to_mask(segmentation: np.ndarray, region_or_label: Union[int, Tuple[int, ...]]) -> np.ndarray:\n","    if np.isscalar(region_or_label):\n","        return segmentation == region_or_label\n","    else:\n","        mask = np.zeros_like(segmentation, dtype=bool)\n","        for r in region_or_label:\n","            mask[segmentation == r] = True\n","    return mask\n","\n","\n","def compute_tp_fp_fn_tn(mask_ref: np.ndarray, mask_pred: np.ndarray, ignore_mask: np.ndarray = None):\n","    if ignore_mask is None:\n","        use_mask = np.ones_like(mask_ref, dtype=bool)\n","    else:\n","        use_mask = ~ignore_mask\n","    tp = np.sum((mask_ref & mask_pred) & use_mask)\n","    fp = np.sum(((~mask_ref) & mask_pred) & use_mask)\n","    fn = np.sum((mask_ref & (~mask_pred)) & use_mask)\n","    tn = np.sum(((~mask_ref) & (~mask_pred)) & use_mask)\n","    return tp, fp, fn, tn\n","\n","\n","def compute_metrics(reference_file: str, prediction_file: str, image_reader_writer: BaseReaderWriter,\n","                    labels_or_regions: Union[List[int], List[Union[int, Tuple[int, ...]]]],\n","                    ignore_label: int = None) -> dict:\n","    # load images\n","    seg_ref, seg_ref_dict = image_reader_writer.read_seg(reference_file)\n","    seg_pred, seg_pred_dict = image_reader_writer.read_seg(prediction_file)\n","\n","    ignore_mask = seg_ref == ignore_label if ignore_label is not None else None\n","\n","    results = {}\n","    results['reference_file'] = reference_file\n","    results['prediction_file'] = prediction_file\n","    results['metrics'] = {}\n","    for r in labels_or_regions:\n","        results['metrics'][r] = {}\n","        mask_ref = region_or_label_to_mask(seg_ref, r)\n","        mask_pred = region_or_label_to_mask(seg_pred, r)\n","        tp, fp, fn, tn = compute_tp_fp_fn_tn(mask_ref, mask_pred, ignore_mask)\n","        if tp + fp + fn == 0:\n","            results['metrics'][r]['Dice'] = np.nan\n","            results['metrics'][r]['IoU'] = np.nan\n","        else:\n","            results['metrics'][r]['Dice'] = 2 * tp / (2 * tp + fp + fn)\n","            results['metrics'][r]['IoU'] = tp / (tp + fp + fn)\n","        results['metrics'][r]['FP'] = fp\n","        results['metrics'][r]['TP'] = tp\n","        results['metrics'][r]['FN'] = fn\n","        results['metrics'][r]['TN'] = tn\n","        results['metrics'][r]['n_pred'] = fp + tp\n","        results['metrics'][r]['n_ref'] = fn + tp\n","    return results\n","\n","\n","def compute_metrics_on_folder(folder_ref: str, folder_pred: str, output_file: str,\n","                              image_reader_writer: BaseReaderWriter,\n","                              file_ending: str,\n","                              regions_or_labels: Union[List[int], List[Union[int, Tuple[int, ...]]]],\n","                              ignore_label: int = None,\n","                              num_processes: int = 6,\n","                              chill: bool = True) -> dict:\n","    \"\"\"\n","    output_file must end with .json; can be None\n","    \"\"\"\n","    if output_file is not None:\n","        assert output_file.endswith('.json'), 'output_file should end with .json'\n","    print('empezando a leer archivos')\n","    files_pred = subfiles(folder_pred, suffix=file_ending, join=False)\n","    files_ref = subfiles(folder_ref, suffix=file_ending, join=False)\n","    print('archivos leidos')\n","    if not chill:\n","        present = [isfile(join(folder_pred, i)) for i in files_ref]\n","        assert all(present), \"Not all files in folder_ref exist in folder_pred\"\n","    print('formando paths')\n","    files_ref = [join(folder_ref, i) for i in files_ref]\n","    files_pred = [join(folder_pred, i) for i in files_pred]\n","    print('paths formandos')\n","    results = []\n","    contador = 0\n","    print('Calculando métricas')\n","    for ref, pred in zip(files_ref, files_pred):\n","      result = compute_metrics(ref, pred, image_reader_writer, regions_or_labels, ignore_label)\n","      results.append(result)\n","      contador += 1\n","      print(contador)\n","\n","    # mean metric per class\n","    metric_list = list(results[0]['metrics'][regions_or_labels[0]].keys())\n","    means = {}\n","    contador = 0\n","    print('Calculando medias')\n","    for r in regions_or_labels:\n","        means[r] = {}\n","        for m in metric_list:\n","            means[r][m] = np.nanmean([i['metrics'][r][m] for i in results])\n","\n","        contador += 1\n","        print(contador)\n","\n","    # foreground mean\n","    foreground_mean = {}\n","    contador = 0\n","    print('Calculando foreground')\n","    for m in metric_list:\n","        values = []\n","        for k in means.keys():\n","            if k == 0 or k == '0':\n","                continue\n","            values.append(means[k][m])\n","        foreground_mean[m] = np.mean(values)\n","        contador += 1\n","        print(contador)\n","\n","    [recursive_fix_for_json_export(i) for i in results]\n","    recursive_fix_for_json_export(means)\n","    recursive_fix_for_json_export(foreground_mean)\n","    result = {'metric_per_case': results, 'mean': means, 'foreground_mean': foreground_mean}\n","    if output_file is not None:\n","        save_summary_json(result, output_file)\n","    return result\n","    # print('DONE')"],"metadata":{"id":"CKjcdZ3jwGZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculo"],"metadata":{"id":"Thun3kXI2lTv"}},{"cell_type":"code","source":["folder_ref = '/content/drive/MyDrive/TFG/Uformer/V2/dataV3/CholecSeg8k/test/y'\n","folder_pred = '/content/drive/MyDrive/TFG/Uformer/V2/logs/CholecSeg8k/Uformer_T_/results_no_color/png'\n","output_file = '/content/drive/MyDrive/TFG/Resultados/metrics/uformer_summary.json'\n","image_reader_writer = NaturalImage2DIO()\n","file_ending = '.png'\n","labels = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n","ignore_label = None\n","num_processes = 6\n","compute_metrics_on_folder(folder_ref, folder_pred, output_file, image_reader_writer, file_ending, labels, ignore_label,\n","                          num_processes)"],"metadata":{"id":"Bbijbi84yWjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["folder_ref = '/content/drive/MyDrive/TFG/Parte2/target_data/test/y'\n","folder_pred = '/content/drive/MyDrive/TFG/Parte2/FineTune/logs/CholecSeg8k/results_no_color/png'\n","output_file = '/content/drive/MyDrive/TFG/Resultados/metrics/reprec_summary.json'\n","image_reader_writer = NaturalImage2DIO()\n","file_ending = '.png'\n","labels = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n","ignore_label = None\n","num_processes = 6\n","compute_metrics_on_folder(folder_ref, folder_pred, output_file, image_reader_writer, file_ending, labels, ignore_label,\n","                          num_processes)"],"metadata":{"id":"UbSw5XQO2yTb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Matriz de confusion"],"metadata":{"id":"4D4hU4Qk8V7G"}},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","def load_images_from_folder(folder):\n","    images = []\n","    for filename in sorted(os.listdir(folder)):\n","        img = io.imread(os.path.join(folder, filename))\n","        if img is not None:\n","            images.append(img)\n","    return images\n","\n","def flatten_images(images):\n","    flattened_images = [img.flatten() for img in images]\n","    return np.concatenate(flattened_images)\n","\n","# Carpetas con las imágenes de predicciones y etiquetas\n","predictions_folder = 'path/to/predictions'\n","labels_folder = 'path/to/labels'\n","\n","# Cargar las imágenes de predicciones y etiquetas\n","predictions = load_images_from_folder(predictions_folder)\n","labels = load_images_from_folder(labels_folder)\n","\n","# Asegurarse de que tienen el mismo número de imágenes\n","assert len(predictions) == len(labels), \"El número de imágenes de predicciones y etiquetas debe ser igual.\"\n","\n","# Aplanar las imágenes\n","predictions_flat = flatten_images(predictions)\n","labels_flat = flatten_images(labels)\n","\n","# Calcular la matriz de confusión\n","conf_matrix = confusion_matrix(labels_flat, predictions_flat)\n","\n","print(\"Matriz de confusión:\")\n","print(conf_matrix)\n","\n"],"metadata":{"id":"WMUKjLLY8Yd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Segmentaciones generadas"],"metadata":{"id":"KFz4pwCq8fCv"}},{"cell_type":"code","source":["def colorear_seg_maps(seg_maps):\n","  seg2 = np.zeros((512, 1024, 3))\n","\n","  seg2[:, :, 0] = seg_maps\n","  seg2[:, :, 1] = seg_maps\n","  seg2[:, :, 2] = seg_maps\n","\n","  pixeles_0 = np.all(seg2 == [0, 0, 0], axis=2)\n","  pixeles_1 = np.all(seg2 == [1, 1, 1], axis=2)\n","  pixeles_2 = np.all(seg2 == [2, 2, 2], axis=2)\n","  pixeles_3 = np.all(seg2 == [3, 3, 3], axis=2)\n","  pixeles_4 = np.all(seg2 == [4, 4, 4], axis=2)\n","  pixeles_5 = np.all(seg2 == [5, 5, 5], axis=2)\n","  pixeles_6 = np.all(seg2 == [6, 6, 6], axis=2)\n","  pixeles_7 = np.all(seg2 == [7, 7, 7], axis=2)\n","  pixeles_8 = np.all(seg2 == [8, 8, 8], axis=2)\n","  pixeles_9 = np.all(seg2 == [9, 9, 9], axis=2)\n","  pixeles_10 = np.all(seg2 == [10, 10, 10], axis=2)\n","  pixeles_11 = np.all(seg2 == [11, 11, 11], axis=2)\n","  pixeles_12 = np.all(seg2 == [12, 12, 12], axis=2)\n","\n","\n","  seg2[pixeles_0] = [127, 127, 127]\n","  seg2[pixeles_1] = [210, 140, 140]\n","  seg2[pixeles_2] = [255, 114, 114]\n","  seg2[pixeles_3] = [231, 70, 156]\n","  seg2[pixeles_4] = [186, 183, 75]\n","  seg2[pixeles_5] = [170, 255, 0]\n","  seg2[pixeles_6] = [255, 85, 0]\n","  seg2[pixeles_7] = [255, 0, 0]\n","  seg2[pixeles_8] = [255, 255, 0]\n","  seg2[pixeles_9] = [169, 255, 184]\n","  seg2[pixeles_10] = [255, 160, 165]\n","  seg2[pixeles_11] = [0, 50, 128]\n","  seg2[pixeles_12] = [111, 74, 0]\n","\n","  return seg2"],"metadata":{"id":"CwYITLddBRx7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num = 7720\n","\n","directorios = {'input': '/content/drive/MyDrive/TFG/V2/nnUNet_raw/Dataset212_CholecSeg8kV2/imagesTs',\n","               'groundtruth': '/content/drive/MyDrive/TFG/V2/evaluationV2/labelsTs_colored',\n","               'nnunet': '/content/drive/MyDrive/TFG/V2/evaluationV2/predictionPP_Colored',\n","               'uformer': '/content/drive/MyDrive/TFG/Uformer/V2/logs/CholecSeg8k/Uformer_T_/results/png',\n","               'reprec': '/content/drive/MyDrive/TFG/Parte2/FineTune/logs/CholecSeg8k/results/png',\n","               }\n","\n","nombres_imagenes = {'input': f'CS_{num}_0000.png',\n","                    'groundtruth': f'CS_{num}.png',\n","                    'nnunet': f'CS_{num}.png'\n","                    'uformer': f'test_prediction-{num}.png',\n","                    'reprec': f'test_prediction-{num}.png',\n","                    }\n","\n","\n","fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n","\n","for i, (tipo, directorio) in enumerate(directorios.items()):\n","    imagen_path = os.path.join(directorio, nombres_imagenes[tipo])\n","    imagen = io.imread(imagen_path)\n","    if tipo == 'groundtruth':\n","      print(imagen.shape)\n","      imagen = colorear_seg_maps(imagen)\n","      imagen = imagen.astype(np.uint8)\n","      imagen = cv2.resize(imagen, (854, 480), interpolation = cv2.INTER_NEAREST)\n","      imagen = imagen.astype(np.uint8)\n","    axs[i].imshow(imagen)\n","    axs[i].set_title(tipo.capitalize())\n","    axs[i].axis('off')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"_ipWsaS38jGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import os\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from skimage import io\n","\n","\n","directorios = {\n","    'input': '/content/drive/MyDrive/TFG/V2/nnUNet_raw/Dataset212_CholecSeg8kV2/imagesTs',\n","    'groundtruth': '/content/drive/MyDrive/TFG/V2/evaluationV2/labelsTs_colored',\n","    'nnunet': '/content/drive/MyDrive/TFG/V2/evaluationV2/predictionPP_Colored',\n","    'uformer': '/content/drive/MyDrive/TFG/Uformer/V2/logs/CholecSeg8k/Uformer_T_/results/png',\n","    'reprec': '/content/drive/MyDrive/TFG/Parte2/FineTune/logs/CholecSeg8k/results/png',\n","}\n","\n","\n","n_examples = 10\n","\n","\n","input_dir = Path(directorios['input'])\n","all_files = list(input_dir.glob('CS_*.png'))\n","all_nums = [int(f.stem.split('_')[1]) for f in all_files]\n","\n","\n","random.seed(75)\n","selected_nums = random.sample(all_nums, n_examples)\n","\n","\n","\n","fig, axs = plt.subplots(n_examples, 5, figsize=((854/225) * 5, (480/225)* n_examples))\n","\n","titles = ['Input', 'Groundtruth', 'nnUnet', 'Uformer', 'RepRec']\n","\n","for col, title in enumerate(titles):\n","    axs[0, col].set_title(title, fontsize=20)\n","    axs[0, col].axis('off')\n","\n","for row, num in enumerate(selected_nums):\n","    nombres_imagenes = {\n","        'input': f'CS_{num:04d}_0000.png',\n","        'groundtruth': f'CS_{num:04d}.png',\n","        'nnunet': f'CS_{num:04d}.png',\n","        'uformer': f'test_prediction-{num:04d}.png',\n","        'reprec': f'test_prediction-{num:04d}.png',\n","    }\n","\n","    for col, (tipo, directorio) in enumerate(directorios.items()):\n","        imagen_path = os.path.join(directorio, nombres_imagenes[tipo])\n","        imagen = io.imread(imagen_path)\n","        axs[row, col].imshow(imagen, aspect='auto')\n","        axs[row, col].axis('off')\n","\n","# Guardar la figura como archivo PNG\n","output_path = '//content/drive/MyDrive/TFG/Resultados/segmentation_examples/10_examples_6.png'\n","#plt.tight_layout(pad=0.5, h_pad=-55)\n","plt.tight_layout()\n","plt.savefig(output_path, format='png', bbox_inches='tight', pad_inches=0)\n","\n","# Mostrar la figura\n","plt.show()"],"metadata":{"id":"PmdGqxm1XBQE"},"execution_count":null,"outputs":[]}]}